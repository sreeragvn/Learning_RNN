{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Recurrent Neural Networks\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Welcome to Project 3: Recurrent Neural Networks!<br>\n",
    "    <br>\n",
    "    In this project you will work with a basic RNN architecture for text generation, namely Sequence to Seqeuence Models. The name points out that these models take a sequence as input and return another sequence as output (see Fig. 1).<br>\n",
    "    <br>\n",
    "    Sequence to Vector Models are similar, but return only a single output in the final time step (see Fig. 2). The latter architecture is, for example, suitable for sequence classification.\n",
    "</div>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <th><img src=\"rnn_seq_2_seq.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "        <th><img src=\"rnn_seq_2_vec.png?666\" alt=\"\" style=\"width: 475px;\"></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fig. 1: Sequence to Sequence RNN with one hidden layer.</th>\n",
    "        <th>Fig. 2: Sequence to Vector RNN with one hidden layer.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 23:46:27.618007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7856400151119191667\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2475294720\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4390755526817733996\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 23:49:37.117088: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-16 23:49:37.159358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:37.244162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:37.244396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:38.046514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:38.046684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:38.046823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:38.046959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 2360 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 23:49:48.247433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:48.247965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:48.248366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:48.248975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:48.249332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 23:49:48.249563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 2360 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Read the text files <font face='courier'>hamlet_1.txt</font>, <font face='courier'>hamlet_2.txt</font> and <font face='courier'>hamlet_3.txt</font>.<br>\n",
    "    <br>\n",
    "    Store their content in variables <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>, respectively.<br>\n",
    "    <br>\n",
    "    Be aware that all files are UTF-8 encoded. Maybe you find <a href='https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files'>this link</a> helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "with open('hamlet_1.txt', encoding=\"utf-8\") as f:\n",
    "    hamlet_1_text = f.read()\n",
    "with open('hamlet_2.txt', encoding=\"utf-8\") as f:\n",
    "    hamlet_2_text = f.read()\n",
    "with open('hamlet_3.txt', encoding=\"utf-8\") as f:\n",
    "    hamlet_3_text = f.read()\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Print the first 325 characters of <font face='courier'>hamlet_1_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      " Barnardo. Who's there?\n",
      "Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      " Bar. Long liue the King\n",
      "\n",
      " Fran. Barnardo?\n",
      "Bar. He\n",
      "\n",
      " Fran. You come most carefully vpon your houre\n",
      "\n",
      " Bar. 'Tis now strook twelue, get thee to bed Francisco\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "print(hamlet_1_text[0:325])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should see the following output:\n",
    "\n",
    "```\n",
    "The Tragedie of Hamlet\n",
    "\n",
    "Actus Primus. Scoena Prima.\n",
    "\n",
    "Enter Barnardo and Francisco two Centinels.\n",
    "\n",
    " Barnardo. Who's there?\n",
    "Fran. Nay answer me: Stand & vnfold\n",
    "your selfe\n",
    "\n",
    " Bar. Long liue the King\n",
    "\n",
    " Fran. Barnardo?\n",
    "Bar. He\n",
    "\n",
    " Fran. You come most carefully vpon your houre\n",
    "\n",
    " Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Instantiate an object <font face='courier'>tokenizer</font> of type <font face='courier'>tf.keras.preprocessing.text.Tokenzier</font>.<br>\n",
    "    <br>\n",
    "    Submit the argument <font face='courier'>char_level=True</font> to the constructor.<br>\n",
    "    <br>\n",
    "    Helpful information can be found <a href='https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer'>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The object <font face='courier'>tokenizer</font> will ultimately be used to encode the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font> as integer sequences in which each single number represents a specific character.<br>\n",
    "    <br>\n",
    "    Call the method <font face='courier'>fit_on_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    In doing so, pass a list that contains exactly the strings <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "data = hamlet_1_text + hamlet_2_text + hamlet_3_text \n",
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The above call creates a vocabulary in <font face='courier'>tokenizer</font>. This vocabulary assigns a positive integer to each unique character in the texts that were passed to the <font face='courier'>fit_on_texts</font> method.<br>\n",
    "    <br>\n",
    "    Display the attribute <font face='courier'>word_index</font> of <font face='courier'>tokenizer</font> which contains the created vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'n': 8, 's': 9, 'r': 10, 'l': 11, '\\n': 12, 'u': 13, 'd': 14, 'm': 15, 'y': 16, ',': 17, 'w': 18, 'f': 19, 'c': 20, 'g': 21, '.': 22, 'p': 23, 'b': 24, 'k': 25, \"'\": 26, ':': 27, 'v': 28, '?': 29, ';': 30, 'q': 31, 'x': 32, '-': 33, 'z': 34, '(': 35, ')': 36, '&': 37, '!': 38, '[': 39, ']': 40, '1': 41, 'j': 42}\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "index = tokenizer.word_index\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have got the following output:\n",
    "\n",
    "```\n",
    "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'n': 8, 's': 9, 'r': 10, 'l': 11, '\\n': 12, 'u': 13, 'd': 14, 'm': 15, 'y': 16, ',': 17, 'w': 18, 'f': 19, 'c': 20, 'g': 21, '.': 22, 'p': 23, 'b': 24, 'k': 25, \"'\": 26, ':': 27, 'v': 28, '?': 29, ';': 30, 'q': 31, 'x': 32, '-': 33, 'z': 34, '(': 35, ')': 36, '&': 37, '!': 38, '[': 39, ']': 40, '1': 41, 'j': 42}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compute the length of the vocabulary, store it in the variable <font face='courier'>max_id</font> and display this variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "max_id = len(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Now, use the method <font face='courier'>texts_to_sequences</font> of <font face='courier'>tokenzier</font> to encode <font face='courier'>hamlet_1_text</font>, <font face='courier'>hamlet_2_text</font> and <font face='courier'>hamlet_3_text</font>.<br>\n",
    "    <br>\n",
    "    Store the respective coded strings in <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.<br>\n",
    "    <br>\n",
    "    Convert all three lists to the format <font face='courier'>numpy.array</font> and subtract <font face='courier'>1</font> from all entries so that all values range between <font face='courier'>0</font> and <font face='courier'>max_id - 1</font> afterwards.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded = tokenizer.texts_to_sequences(hamlet_1_text)\n",
    "hamlet_2_encoded = tokenizer.texts_to_sequences(hamlet_2_text)\n",
    "hamlet_3_encoded = tokenizer.texts_to_sequences(hamlet_3_text)\n",
    "\n",
    "hamlet_1_encoded = np.hstack(hamlet_1_encoded)\n",
    "hamlet_1_encoded = hamlet_1_encoded-1\n",
    "hamlet_2_encoded = np.hstack(hamlet_2_encoded)\n",
    "hamlet_2_encoded = hamlet_2_encoded-1\n",
    "hamlet_3_encoded = np.hstack(hamlet_3_encoded)\n",
    "hamlet_3_encoded = hamlet_3_encoded-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first <font face='courier'>325</font> entries of <font face='courier'>hamlet_1_encoded</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
      "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
      " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
      " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
      " 11 11  0 23  4  9  7  4  9 13  3 21  0 17  5  3 25  8  0  2  5  1  9  1\n",
      " 28 11 18  9  4  7 21  0  7  4 15  0  4  7  8 17  1  9  0 14  1 26  0  8\n",
      "  2  4  7 13  0 36  0 27  7 18  3 10 13 11 15  3 12  9  0  8  1 10 18  1\n",
      " 11 11  0 23  4  9 21  0 10  3  7 20  0 10  6 12  1  0  2  5  1  0 24  6\n",
      "  7 20 11 11  0 18  9  4  7 21  0 23  4  9  7  4  9 13  3 28 11 23  4  9\n",
      " 21  0  5  1 11 11  0 18  9  4  7 21  0 15  3 12  0 19  3 14  1  0 14  3\n",
      "  8  2  0 19  4  9  1 18 12 10 10 15  0 27 22  3  7  0 15  3 12  9  0  5\n",
      "  3 12  9  1 11 11  0 23  4  9 21  0 25  2  6  8  0  7  3 17  0  8  2  9\n",
      "  3  3 24  0  2 17  1 10 12  1 16  0 20  1  2  0  2  5  1  1  0  2  3  0\n",
      " 23  1 13  0 18  9  4  7 19  6  8 19  3]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "print(hamlet_1_encoded[0:325])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should be as follows:\n",
    "\n",
    "```\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4  9  7  4  9 13  3 21  0 17  5  3 25  8  0  2  5  1  9  1\n",
    " 28 11 18  9  4  7 21  0  7  4 15  0  4  7  8 17  1  9  0 14  1 26  0  8\n",
    "  2  4  7 13  0 36  0 27  7 18  3 10 13 11 15  3 12  9  0  8  1 10 18  1\n",
    " 11 11  0 23  4  9 21  0 10  3  7 20  0 10  6 12  1  0  2  5  1  0 24  6\n",
    "  7 20 11 11  0 18  9  4  7 21  0 23  4  9  7  4  9 13  3 28 11 23  4  9\n",
    " 21  0  5  1 11 11  0 18  9  4  7 21  0 15  3 12  0 19  3 14  1  0 14  3\n",
    "  8  2  0 19  4  9  1 18 12 10 10 15  0 27 22  3  7  0 15  3 12  9  0  5\n",
    "  3 12  9  1 11 11  0 23  4  9 21  0 25  2  6  8  0  7  3 17  0  8  2  9\n",
    "  3  3 24  0  2 17  1 10 12  1 16  0 20  1  2  0  2  5  1  1  0  2  3  0\n",
    " 23  1 13  0 18  9  4  7 19  6  8 19  3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The original texts can be recovered via the method <font face='courier'>sequences_to_texts</font> of <font face='courier'>tokenizer</font>.<br>\n",
    "    <br>\n",
    "    Texts recovered in this way are all lower case and each original character is followed by a blank space.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>sequences_to_texts</font> to <font face='courier'>hamlet_1_encoded + 1</font> and store the result in <font face='courier'>hamlet_1_decoded</font>.<br>\n",
    "    <br>\n",
    "    Afterfwards, display the first <font face='courier'>649</font> characters of <font face='courier'>hamlet_1_decoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e   t r a g e d i e   o f   h a m l e t \n",
      " \n",
      " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
      " \n",
      " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
      " \n",
      "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
      " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
      " y o u r   s e l f e \n",
      " \n",
      "   b a r .   l o n g   l i u e   t h e   k i n g \n",
      " \n",
      "   f r a n .   b a r n a r d o ? \n",
      " b a r .   h e \n",
      " \n",
      "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
      " \n",
      "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded = np.vstack(hamlet_1_encoded)\n",
    "hamlet_1_encoded = hamlet_1_encoded + 1\n",
    "hamlet_1_decoded = tokenizer.sequences_to_texts(hamlet_1_encoded)\n",
    "print(' '.join(hamlet_1_decoded)[:649])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look like this:\n",
    "\n",
    "```\n",
    "t h e   t r a g e d i e   o f   h a m l e t \n",
    " \n",
    " a c t u s   p r i m u s .   s c o e n a   p r i m a . \n",
    " \n",
    " e n t e r   b a r n a r d o   a n d   f r a n c i s c o   t w o   c e n t i n e l s . \n",
    " \n",
    "   b a r n a r d o .   w h o ' s   t h e r e ? \n",
    " f r a n .   n a y   a n s w e r   m e :   s t a n d   &   v n f o l d \n",
    " y o u r   s e l f e \n",
    " \n",
    "   b a r .   l o n g   l i u e   t h e   k i n g \n",
    " \n",
    "   f r a n .   b a r n a r d o ? \n",
    " b a r .   h e \n",
    " \n",
    "   f r a n .   y o u   c o m e   m o s t   c a r e f u l l y   v p o n   y o u r   h o u r e \n",
    " \n",
    "   b a r .   ' t i s   n o w   s t r o o k   t w e l u e ,   g e t   t h e e   t o   b e d   f r a n c i s c o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Create three objects <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> of type <font face='courier'>tf.data.dataset</font> by applying the method<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.from_tensor_slices</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices'>this link</a>) to <font face='courier'>hamlet_1_encoded</font>, <font face='courier'>hamlet_2_encoded</font> and <font face='courier'>hamlet_3_encoded</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_encoded = np.hstack(hamlet_1_encoded)\n",
    "hamlet_1_encoded = hamlet_1_encoded-1\n",
    "\n",
    "hamlet_1_dataset = tf.data.Dataset.from_tensor_slices(hamlet_1_encoded)\n",
    "hamlet_2_dataset = tf.data.Dataset.from_tensor_slices(hamlet_2_encoded)\n",
    "hamlet_3_dataset = tf.data.Dataset.from_tensor_slices(hamlet_3_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first ten elements of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "for element in hamlet_1_dataset.take(10):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have produced the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, each item of <font face='courier'>hamlet_1_dataset</font> is an integer tensor including one single value.<br>\n",
    "    <br>\n",
    "    Accordingly, <font face='courier'>hamlet_1_dataset</font> has <font face='courier'>len(hamlet_1_encoded)</font> elements in total (the same applies in case of the other two datasets).<br>\n",
    "    <br>\n",
    "    In the following, you will train a recurrent neural network which gets a coded string of length <font face='courier'>T = 100</font> and predicts subsequent characters in all time steps.<br>\n",
    "    <br>\n",
    "    Accordingly, we will first transform our three datasets such that their elements become one-dimensional tensors of length <font face='courier'>window_length = T + 1</font>.<br>\n",
    "    <br>\n",
    "    Initialize <font face='courier'>T</font> and <font face='courier'>window_length</font> as described.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "T = 100\n",
    "window_length = T + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use the method <font face='courier'>tf.data.Dataset.window</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window'>this link</a>) to get items that feature the desired length.<br>\n",
    "    <br>\n",
    "    Call the method using the arguments <font face='courier'>shift = 1</font> and <font face='courier'>drop_remainder = True</font>.<br>\n",
    "    <br>\n",
    "    The first-mentioned argument <font face='courier'>shift = 1</font> ensures that the first item of the transformed dataset contains the elements <font face='courier'>0,...,window_length - 1</font> of the original dataset, the next item <font face='courier'>1,...,window_length</font>, and so on.<br>\n",
    "    <br>\n",
    "    In other words: A window of length <font face='courier'>window_length</font> slides with feed <font face='courier'>shift</font> over the original dataset and extracts sequence by sequence until the end of the dataset is reached.<br>\n",
    "    <br>\n",
    "    The latter argument <font face='courier'>drop_remainder = True</font> ensures that no shorter sequences are extracted towards the end of the dataset, where the window could potentially slide beyond the end of the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = hamlet_1_dataset.window(size=window_length,shift = 1, drop_remainder = True)\n",
    "hamlet_2_dataset = hamlet_2_dataset.window(size=window_length,shift = 1, drop_remainder = True)\n",
    "hamlet_3_dataset = hamlet_3_dataset.window(size=window_length,shift = 1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for window in hamlet_1_dataset.take(1):\n",
    "    print(window)\n",
    "    for item in window.take(10):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should have obtained an output like this:\n",
    "\n",
    "```\n",
    "<_VariantDataset shapes: (), types: tf.int32>\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(5, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(0, shape=(), dtype=int32)\n",
    "tf.Tensor(2, shape=(), dtype=int32)\n",
    "tf.Tensor(9, shape=(), dtype=int32)\n",
    "tf.Tensor(4, shape=(), dtype=int32)\n",
    "tf.Tensor(20, shape=(), dtype=int32)\n",
    "tf.Tensor(1, shape=(), dtype=int32)\n",
    "tf.Tensor(13, shape=(), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    As you can see, the transformed datasets <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> have now elements which are again objects of type <font face='courier'>tf.data.Dataset</font> (or of a derived class).<br>\n",
    "    <br>\n",
    "    Each of these sub-datasets <font face='courier'>window</font> contains a number of <font face='courier'>window_size</font> single-valued tensors.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.flat_map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map'>this link</a>) to all three datasets to transform the sub-datasets <font face='courier'>window</font> into one-dimensional tensors of length <font face='courier'>window_length</font>.<br>\n",
    "    <br>\n",
    "    Pass a function which maps <font face='courier'>window</font> to <font face='courier'>window.batch(window_length)</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_1_dataset = hamlet_1_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "hamlet_2_dataset = hamlet_2_dataset.flat_map(lambda window: window.batch(window_length))\n",
    "hamlet_3_dataset = hamlet_3_dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Display the first element of <font face='courier'>hamlet_1_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
      "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
      " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
      " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
      " 11 11  0 23  4], shape=(101,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "for i in hamlet_1_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should get the following output:\n",
    "\n",
    "```\n",
    "tf.Tensor(\n",
    "[ 2  5  1  0  2  9  4 20  1 13  6  1  0  3 18  0  5  4 14 10  1  2 11 11\n",
    "  4 19  2 12  8  0 22  9  6 14 12  8 21  0  8 19  3  1  7  4  0 22  9  6\n",
    " 14  4 21 11 11  1  7  2  1  9  0 23  4  9  7  4  9 13  3  0  4  7 13  0\n",
    " 18  9  4  7 19  6  8 19  3  0  2 17  3  0 19  1  7  2  6  7  1 10  8 21\n",
    " 11 11  0 23  4], shape=(101,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.concatenate</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#concatenate'>this link</a>) to merge<br>\n",
    "    <br>\n",
    "    <font face='courier'>hamlet_1_dataset</font>, <font face='courier'>hamlet_2_dataset</font> and <font face='courier'>hamlet_3_dataset</font> to a single dataset <font face='courier'>hamlet_dataset</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset= (hamlet_1_dataset.concatenate(hamlet_2_dataset)).concatenate(hamlet_3_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Set <font face='courier'>batch_size = 32</font>.<br>\n",
    "    <br>\n",
    "    Apply <font face='courier'>tf.data.Dataset.repeat</font> (without argument),<br>\n",
    "    <br>\n",
    "    <font face='courier'>tf.data.Dataset.shuffle</font> (with <font face='courier'>buffer_size = 1000</font>)<br>\n",
    "    <br>\n",
    "    and finally <font face='courier'>tf.data.Dataset.batch</font> (with <font face='courier'>drop_remainder=True</font>).\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# YOUR CODE\n",
    "batch_size = 32\n",
    "hamlet_dataset = hamlet_dataset.repeat()\n",
    "hamlet_dataset = hamlet_dataset.shuffle(buffer_size=10000)\n",
    "hamlet_dataset = hamlet_dataset.batch(batch_size=batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Our dataset contains now two-dimensional tensors <font face='courier'>window_batch</font> of size <font face='courier'>(32, 101)</font>.<br>\n",
    "    <br>\n",
    "    Each slice <font face='courier'>window_batch[i, :]</font> corresponds to a training example.<br>\n",
    "    <br>\n",
    "    Here, we still need to subdivide the training examples into inputs and outputs.<br>\n",
    "    <br>\n",
    "    Each single encoded character <font face='courier'>window_batch[i, j]</font> (for <font face='courier'>j=0,...,99</font>) is an  input $\\mathbf{x}^{<t>(i)}$ in a time step<br>\n",
    "    <br>\n",
    "    and the associated output is <font face='courier'>window_batch[i, j + 1]</font> which corresponds to $\\mathbf{y}^{<t>(i)}$.<br>\n",
    "    <br>\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.map</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map'>this link</a>) to <font face='courier'>hamlet_dataset</font><br>.\n",
    "    <br>\n",
    "    Each batch <font face='courier'>window_batch</font> shall be mapped to a tuple of two tensors of size <font face='courier'>(32, 100)</font>.<br>\n",
    "    <br>\n",
    "    The <font face='courier'>[i, :]</font>-th slice of the first tensor shall contain the entries <font face='courier'>window_batch[i, 0:100]</font>.<br>\n",
    "    <br>\n",
    "    The corresponding slice of the second tensor shall contain <font face='courier'>window_batch[i, 1:101]</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset= hamlet_dataset.map(lambda window_batch: (window_batch[:, 0:100], window_batch[:, 1:101]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Execute the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  o f   y o u n g   f o r t i n b r a s , \n",
      " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
      " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p\n",
      "\n",
      "o f   y o u n g   f o r t i n b r a s , \n",
      " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
      " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p r\n"
     ]
    }
   ],
   "source": [
    "for window_batch in hamlet_dataset.take(1):\n",
    "    [x] = tokenizer.sequences_to_texts([window_batch[0][0, :].numpy() + 1])\n",
    "    [y] = tokenizer.sequences_to_texts([window_batch[1][0, :].numpy() + 1])\n",
    "    print(x)\n",
    "    print()\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Your output should look as follows:\n",
    "\n",
    "```\n",
    "  o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p\n",
    "\n",
    "o f   y o u n g   f o r t i n b r a s , \n",
    " w h o   i m p o t e n t   a n d   b e d r i d ,   s c a r s e l y   h e a r e s \n",
    " o f   t h i s   h i s   n e p h e w e s   p u r p o s e ,   t o   s u p p r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply <font face='courier'>tf.data.Dataset.map</font> again to <font face='courier'>hamlet_dataset</font> to map each element <font face='courier'>(X_batch, Y_batch)</font> to a new tuple.<br>\n",
    "    <br>\n",
    "    In the new tuple, <font face='courier'>Y_batch</font> shall remain unchanged while <font face='courier'>X_batch</font> undergoes another encoding via <font face='courier'>tf.one_hot</font> (see <a href='https://www.tensorflow.org/api_docs/python/tf/one_hot'>this link</a>).<br>\n",
    "    <br>\n",
    "    Find out, which value <font face='courier'>depth</font> you need to pass <font face='courier'>tf.one_hot</font> in addition to <font face='courier'>X_batch</font>.<br>\n",
    "    <br>\n",
    "    Hint: You already computed the correct value above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100, 42), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
      "array([[ 0, 17,  5, ...,  6, 19,  5],\n",
      "       [ 0,  5,  4, ...,  7,  0,  6],\n",
      "       [ 8, 22,  6, ...,  6,  2, 19],\n",
      "       ...,\n",
      "       [13,  0, 14, ..., 21,  0,  4],\n",
      "       [19,  5,  0, ...,  5,  2,  0],\n",
      "       [ 2,  0, 23, ...,  4, 10, 10]])>)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset= hamlet_dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "for i in hamlet_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Apply the method <font face='courier'>tf.data.Dataset.prefetch</font> (with <font face='courier'>buffer_size = 1</font>) to <font face='courier'>hamlet_dataset</font> to get your dataset ready for training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "hamlet_dataset= hamlet_dataset.prefetch(buffer_size= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Deduce a formula for the number of elements in <font face='courier'>hamlet_dataset</font>.<br>\n",
    "    <br>\n",
    "    Display the result and store it in <font face='courier'>steps_per_epoch</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5014\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "steps_per_epoch= int(((len(hamlet_1_encoded)+len(hamlet_2_encoded)+len(hamlet_3_encoded))-3*T)/batch_size)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The number of items in the dataset should be as follows:\n",
    "\n",
    "```\n",
    "5014\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Use <font face='courier'>keras.models.Sequential</font> to define a <font face='courier'>model</font> featuring<br>\n",
    "    <br>\n",
    "    two hidden GRU layers (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU'>this link</a>) with <font face='courier'>128</font> neurons each, as well as<br>\n",
    "    <br>\n",
    "    a fully connected output layer (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'>this link</a>) with <font face='courier'>max_id</font> neurons and <font face='courier'>softmax</font> activation function.<br>\n",
    "    <br>\n",
    "    This model corresponds to the RNN displayed in Fig. 1 with an additional hidden GRU layer.<br>\n",
    "    <br>\n",
    "    To make the model generate outputs in each time step, the output layer must be enclosed by a <font face='courier'>keras.layers.TimeDistributed</font> wrapper (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed'>this link</a>).<br>\n",
    "    <br>\n",
    "    Without the layer, the model would generate only a single output in the very last time step (as the one displaye in Fig. 2).<br>\n",
    "    <br>\n",
    "    In case of the GRU layers, you should add <font face='courier'>return_sequences = True</font> for the same purpose.<br>\n",
    "    <br>\n",
    "    Also recall that the input layer needs an argument <font face='courier'>input_shape=[None, max_id]</font> (where <font face='courier'>None</font> represents the temporal dimension of the input sequence).<br>\n",
    "    <br>\n",
    "    During training, we could just as well replace <font face='courier'>None</font> with <font face='courier'>T</font> as all sequences in the training data have identical length.<br>\n",
    "    <br>\n",
    "    However, passing <font face='courier'>None</font> ensures that the model will accept arbitrarily long sequences later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "model=keras.models.Sequential([keras.layers.GRU(128,return_sequences=True, input_shape=[None,max_id]),keras.layers.GRU(128,return_sequences=True),keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Compile <font face='courier'>model</font> using <font face='courier'>sparse_categorical_crossentropy</font> and <font face='courier'>adam</font>.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Train <font face='courier'>model</font> for one epoch. Don't forget to pass the <font face='courier'>steps_per_epoch</font> argument.\n",
    "    <a href=''></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "model.fit(hamlet_dataset, epochs=1, steps_per_epoch= steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    You might have observed that even a single epoch takes quite some time.<br>\n",
    "    <br>\n",
    "    Create a file <font face='courier'>hamlet_rnn.py</font> and train the model for <font face='courier'>20</font> epochs on the GPU cluster.<br>\n",
    "    <br>\n",
    "    In doing so, use a callback<br>\n",
    "    <br>\n",
    "    <font face='courier'>keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)</font><br>\n",
    "    <br>\n",
    "    (see <a href='https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping'>this link</a>). Save your model as  <font face='courier'>hamlet_model.h5</font> and load it in the following step for further use in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5014/5014 [==============================] - 89s 15ms/step - loss: 1.2277\n",
      "Epoch 2/20\n",
      "5014/5014 [==============================] - 76s 15ms/step - loss: 0.8818\n",
      "Epoch 3/20\n",
      "5014/5014 [==============================] - 75s 15ms/step - loss: 0.7692\n",
      "Epoch 4/20\n",
      "5014/5014 [==============================] - 78s 15ms/step - loss: 0.6958\n",
      "Epoch 5/20\n",
      "5014/5014 [==============================] - 75s 15ms/step - loss: 0.6398\n",
      "Epoch 6/20\n",
      "5014/5014 [==============================] - 75s 15ms/step - loss: 0.5959\n",
      "Epoch 7/20\n",
      "5014/5014 [==============================] - 75s 15ms/step - loss: 0.5585\n",
      "Epoch 8/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.5296\n",
      "Epoch 9/20\n",
      "5014/5014 [==============================] - 73s 15ms/step - loss: 0.5021\n",
      "Epoch 10/20\n",
      "5014/5014 [==============================] - 73s 15ms/step - loss: 0.4778\n",
      "Epoch 11/20\n",
      "5014/5014 [==============================] - 72s 14ms/step - loss: 0.4543\n",
      "Epoch 12/20\n",
      "5014/5014 [==============================] - 73s 14ms/step - loss: 0.4343\n",
      "Epoch 13/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.4141\n",
      "Epoch 14/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.3952\n",
      "Epoch 15/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.3784\n",
      "Epoch 16/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.3671\n",
      "Epoch 17/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.3552\n",
      "Epoch 18/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.3454\n",
      "Epoch 19/20\n",
      "5014/5014 [==============================] - 73s 15ms/step - loss: 0.3345\n",
      "Epoch 20/20\n",
      "5014/5014 [==============================] - 74s 15ms/step - loss: 0.3276\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "history= model.fit(hamlet_dataset, epochs=20, steps_per_epoch= steps_per_epoch,callbacks=[callback])\n",
    "model.save('hamlet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Write a function <font face='courier'>preprocess</font> that takes a argument a list <font face='courier'>texts</font> containing a single string.<br>\n",
    "    <br>\n",
    "    Inside this function, use <font face='courier'>tokenizer</font> again to encode the string in <font face='courier'>texts</font> as a NumPy-Array <font face='courier'>X</font>.<br>\n",
    "    <br>\n",
    "    The return value of <font face='courier'>preprocess</font> shall be the one-hot encoded version of <font face='courier'>X</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "model= tf.keras.models.load_model('hamlet_model.h5')\n",
    "def preprocess(texts):\n",
    "    x = tokenizer.texts_to_sequences(texts)\n",
    "    x = tf.one_hot(x,depth=max_id)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>next_char</font> takes as argument a string <font face='courier'>text</font> and a positive number <font face='courier'>temperature</font><br>\n",
    "    <br>\n",
    "    with the goal to generate the next character after <font face='courier'>text</font>.<br>\n",
    "    <br>\n",
    "    Replace all placeholders <font face='courier'>None</font> as follows:<br>\n",
    "    <br>\n",
    "    First, apply <font face='courier'>preprocess</font> to encode <font face='courier'>text</font> and store the result in <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Second, apply <font face='courier'>model.predict</font> the generate outputs given the input sequence <font face='courier'>X_new</font>.<br>\n",
    "    <br>\n",
    "    Store the output from the last time step in <font face='courier'>y_proba</font>.<br>\n",
    "    <br>\n",
    "    <font face='courier'>rescaled_logits</font> and <font face='courier'>char_id</font> are finally used to generate a one-element sample from  $\\{1,\\dots,\\mathrm{max\\_id}\\}$.<br>\n",
    "    <br>\n",
    "    The generated sample is the encoded version of the character to be generated.<br>\n",
    "    <br>\n",
    "    Hence, <font face='courier'>char_id.numpy()</font> will return the character itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005/2005 [==============================] - 4s 2ms/step\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_19308\\967336109.py\", line 8, in <module>\n",
      "    next_char(hamlet_1_text)\n",
      "  File \"C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_19308\\967336109.py\", line 5, in next_char\n",
      "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 7209, in raise_from_not_ok_status\n",
      "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Multinomial_device_/job:localhost/replica:0/task:0/device:GPU:0}} logits should be a matrix, got shape [64148,1,42] [Op:Multinomial]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"C:\\Users\\patel\\anaconda3\\envs\\gpu11.2\\lib\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = model.predict(X_new)\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "next_char(hamlet_1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The function <font face='courier'>complete_text</font> takes a string <font face='courier'>text</font> and shall append <font face='courier'>n_char</font> subsequent characters generated by your RNN.<br>\n",
    "    <br>\n",
    "    Complete the function accordingly. The argument <font face='courier'>temperature</font> just needs to be passed to <font face='courier'>next_char</font>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text,temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Starting from the initial string <font face='courier'>'Hamlet'</font>, use <font face='courier'>complete_text</font> to generate a text with <font face='courier'>1000</font> characters.<br>\n",
    "    <br>\n",
    "    You can vary the argument <font face='courier'>temperature</font> to modify the distribution from which new characters are drawn.<br>\n",
    "    <br>\n",
    "    Values close to zero encourage characters that have a high probability according to the distribution generated by your RNN.<br>\n",
    "    <br>\n",
    "    If <font face='courier'>temperature</font> is too high, new characters are drawn according to a uniform distribution on the entire vocabulary, which is not desirable.<br>\n",
    "    <br>\n",
    "    You can, for example, try different values between <font face='courier'>0</font> and <font face='courier'>2</font> and evaluate generated texts based on how plausible they appear to you.<br>\n",
    "    <br>\n",
    "    Display your generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "complete_text(hamlet_1_text, n_chars=1000, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Submit your completed notebook not later than on January 15th, 2023.<br>\n",
    "    <br>\n",
    "    Optional: Train an RNN on a text corpus of your own choice and use your own RNN to generate text.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a6b8be28555d0d38b56f139933147c844a524653a76ce05576e7503816385fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
